{"nbformat": 4, "metadata": {"kernelspec": {"language": "R", "display_name": "R", "name": "ir"}, "language_info": {"version": "3.4.2", "mimetype": "text/x-r-source", "pygments_lexer": "r", "codemirror_mode": "r", "name": "R", "file_extension": ".r"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "71cd076012e17f7105f8efbf87395e38e6c000ff", "_cell_guid": "2d73b3a8-f238-4c53-a8b6-28bd55aebdb6"}, "source": ["# Machine learning tutorial: R edition\n", "\n", "I developed this tutorial for a presentation I was giving to the University of Guelph Integrative Biology R users group. The topic was an introduction to the implementation of machine learning algorithms in R.\n", "\n", "### Who can benefit from this?\n", "\n", "This tutorial is a good first step for someone looking to learn the steps needed for exploring data, cleaning data, and training/evaluating some basic machine learning algorithms. It is also a useful resource for someone who is comfortable doing data science in other languages such as python and wants to learn how to apply their data science skills in R. As a fun exercise you could compare this code to the python code in the book listed below.\n", "\n", "Data and code come from chapter 2 of this book: https://github.com/ageron/handson-ml\n", "\n", "Here I have 'translated' (and heavily abridged) the code from python into R so that it can be used as a good intro example for how to implement some machine learning algorithms. The workflow isn't exactly the same as the book but the data arrives cleaned at roughly the same point. \n", "\n", "I've chosen this dataset because:\n", "1. It is freely avaliable online so we won't get sued.\n", "2. It is 'medium' sized. Not small enough to feel overly toyish, but not so big as to be cumbersome.\n", "3. There are a reasonable number of predictor columns, so it isn't too much to take in and understand what they all mean.\n", "\n", "The columns are as follows, their names are pretty self explanitory:\n", "\n", "longitude\n", "\n", "latitude\n", "\n", "housing_median_age\n", "\n", "total_rooms\n", "\n", "total_bedrooms\n", "\n", "population\n", "\n", "households\n", "\n", "median_income\n", "\n", "median_house_value\n", "\n", "ocean_proximity\n", "\n", "Each row pertains to a group of houses (I forget if this is by block or postal code but the important bit is they are medians because it is a bunch of houses in close proximity grouped together).\n", "\n", "## Step 1. Load in the data.\n", "\n", "If you missed the email link, download 'housing.csv' from here:\n", "\n", "https://github.com/ageron/handson-ml/tree/master/datasets/housing\n", "\n", "Then adjust the following code to your directory of choice."]}, {"cell_type": "code", "metadata": {"_uuid": "1e11ce24acc57db026dcf64deee4b5c8ffe575ef", "_cell_guid": "2214c98b-0a2f-41fc-8d79-5819057b8e37"}, "execution_count": null, "outputs": [], "source": ["library(tidyverse)\n", "library(reshape2)"]}, {"cell_type": "code", "metadata": {"_uuid": "eeacb4e81bf2556968a760657948efda044d6f5d", "_cell_guid": "20b0e5f8-2201-44d4-a6c6-15f3a75eaa3e"}, "execution_count": null, "outputs": [], "source": ["housing = read.csv('../housing.csv')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f930d8541aaf0dfbd81c3615ab18003d45384d8a", "_cell_guid": "1155e3bd-5675-4f1e-bcbf-ee8c8983eb36"}, "source": ["First thing I always do is use the head command to make sure the data isn't weird and looks how I expected."]}, {"cell_type": "code", "metadata": {"_uuid": "fca2816bd6240042c2137d8e0506edf92db43452", "_cell_guid": "c0f3b9d4-9cfe-400d-a58f-d7d1c9bcdfd7"}, "execution_count": null, "outputs": [], "source": ["head(housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7c31c2e15680ca44c95ab5181ac19efdbc73ce21", "_cell_guid": "5b7b43d9-1fbd-452c-b2bb-24fd861c9cc1"}, "source": ["Next I always call summary, just to see if the #s are #s and the categoricals are categoricals."]}, {"cell_type": "code", "metadata": {"_uuid": "d59608c1b0476e1b44285cb3e8aafa89a3b8d506", "_cell_guid": "7f30a704-6790-4387-81f1-dbfa242d1011"}, "execution_count": null, "outputs": [], "source": ["summary(housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "63252bfee9c1ff93e06d452166eaf5da2c7794cd", "_cell_guid": "f4f00a78-328e-4d90-ba7f-2b1a9795ed9b"}, "source": ["So from that summary we can see a few things we need to do before actually running algorithms.\n", "1. NA's in total_bedrooms need to be addressed. These must be given a value\n", "2. We will split the ocean_proximity into binary columns. Most machine learning algorithms in R can handle categoricals in a single column, but we will cater to the lowest common denominator and do the splitting.\n", "3. Make the total_bedrooms and total_rooms into a mean_number_bedrooms and mean_number_rooms columns as there are likely more accurate depections of the houses in a given group.\n"]}, {"cell_type": "code", "metadata": {"_uuid": "08f427f7a11af2d6d354d9a806045b109e56afad", "_cell_guid": "86de0f7b-4408-4921-9578-9d58623cacf1"}, "execution_count": null, "outputs": [], "source": ["par(mfrow=c(2,5))"]}, {"cell_type": "code", "metadata": {"_uuid": "4cd8072cff18ca6add2de28911b72dab7a0fc37d", "_cell_guid": "62eeef69-c434-40ca-a678-560b6b4c15ae"}, "execution_count": null, "outputs": [], "source": ["colnames(housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "1fc06b0bbb816cf509cd8711d723ef611ad81bd3", "_cell_guid": "3c13af2c-6f24-40dc-90cf-43fadc85a837"}, "source": ["Lets take a gander at the variables"]}, {"cell_type": "code", "metadata": {"_uuid": "91ea87cab5c51856b46a73032615e14419a961cd", "_cell_guid": "7939f990-4b1d-4027-8f7c-92a0735bd22d"}, "execution_count": null, "outputs": [], "source": ["ggplot(data = melt(housing), mapping = aes(x = value)) + \n", "    geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "96e31d3fdc2a8710a9bccdc69401600cfa433e03", "_cell_guid": "876c13a7-f31f-4f9d-bfd8-3a7199bbe633"}, "source": ["Things I see from this:\n", "1. There are some housing blocks with old age homes in them.\n", "2. The median house value has some weird cap applied to it causing there to be a blip at the rightmost point on the hist. There are most definitely houses in the bay area worth more than 500,000... even in the 90s when this data was collected!\n", "3. We should standardize the scale of the data for any non-tree based methods. As some of the variables range from 0-10, while others go up to 500,000\n", "4. We need to think about how the cap on housing prices can affect our prediction... may be worth removing the capped values and only working with the data we are confident in."]}, {"cell_type": "markdown", "metadata": {"_uuid": "c587617939cccdc8037d5f00fd409685dc1f7486", "_cell_guid": "563fd076-b09c-408c-bf36-577103d1dc9a"}, "source": ["## Step 2. Clean the data\n", "\n", "### Impute missing values\n", "Fill median for total_bedrooms which is the only column with missing values. The median is used instead of mean because it is less influenced by extreme outliers. Note this may not be the best, as these could be actual buildings with no bedrooms (warehouses or something). We don't know... but imputation is often the best of a bad job"]}, {"cell_type": "code", "metadata": {"_uuid": "3d1efcb520732ff2cd5314377d7583af16770b21", "_cell_guid": "5d98ca58-f131-48f1-b377-dcaf59c69402"}, "execution_count": null, "outputs": [], "source": ["housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b69fcde8e79e0d42bf57e96ef53ee4cb6afbc2da", "_cell_guid": "85b7bc1e-0454-4864-a3c0-9e47c5188e3b"}, "source": ["### Fix the total columns - make them means"]}, {"cell_type": "code", "metadata": {"_uuid": "cb973c6cc521ce44cf547d29aefd9977a2440a9c", "_cell_guid": "4b3acbbb-2279-47f2-9f4c-c1131b2ac20d"}, "execution_count": null, "outputs": [], "source": ["housing$mean_bedrooms = housing$total_bedrooms/housing$households\n", "housing$mean_rooms = housing$total_rooms/housing$households\n", "\n", "drops = c('total_bedrooms', 'total_rooms')\n", "\n", "housing = housing[ , !(names(housing) %in% drops)]"]}, {"cell_type": "code", "metadata": {"_uuid": "ab956ed7c32a8bf0c3718c6e2c6e73467d61d8ca", "_cell_guid": "3fdf29f6-bb20-4afd-9011-50f505e649b1"}, "execution_count": null, "outputs": [], "source": ["head(housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "55af40745dfb64a35e7ad9d1f0dc012a150ee423", "_cell_guid": "c3110782-612c-44b1-bec2-b0b2cc159951"}, "source": ["### Turn categoricals into booleans\n", "\n", "Below I do the following:\n", "1. Get a list of all the categories in the 'ocean_proximity' column\n", "2. Make a new empty dataframe of all 0s, where each category is its own colum\n", "3. Use a for loop to populate the appropriate columns of the dataframe\n", "4. Drop the original column from the dataframe.\n", "\n", "This is an example of me coding R with a python accent... I would love comments about how to do this more cleanly in R!\n", "\n", "Fun follow up task: can you turn this into a function that could be used to split any categorial column?"]}, {"cell_type": "code", "metadata": {"_uuid": "affa2352292d74f45dc64506e6b47d6b36d6a432", "_cell_guid": "e9632bbb-1864-42e2-8a34-4914ff99a410"}, "execution_count": null, "outputs": [], "source": ["categories = unique(housing$ocean_proximity)\n", "#split the categories off\n", "cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)"]}, {"cell_type": "code", "metadata": {"_uuid": "2cb5a951461e1d3598640d98b2bea62098f57b70", "_cell_guid": "e22019ff-a7b7-46d1-983d-c89dfe3a54a4"}, "execution_count": null, "outputs": [], "source": ["for(cat in categories){\n", "    cat_housing[,cat] = rep(0, times= nrow(cat_housing))\n", "}\n", "head(cat_housing) #see the new columns on the right"]}, {"cell_type": "code", "metadata": {"_uuid": "e94484af3a671783e45a19d4ce2235af0279dd88", "_cell_guid": "f4122994-46c8-4199-8a26-2f6265d342bd"}, "execution_count": null, "outputs": [], "source": ["for(i in 1:length(cat_housing$ocean_proximity)){\n", "    cat = as.character(cat_housing$ocean_proximity[i])\n", "    cat_housing[,cat][i] = 1\n", "}\n", "\n", "head(cat_housing)"]}, {"cell_type": "code", "metadata": {"_uuid": "f13846f0f3ebbf39dede3f37ee214797a2077add", "_cell_guid": "8ccfe7bf-70cb-4c86-b4ab-809bd1327957"}, "execution_count": null, "outputs": [], "source": ["cat_columns = names(cat_housing)\n", "keep_columns = cat_columns[cat_columns != 'ocean_proximity']\n", "cat_housing = select(cat_housing,one_of(keep_columns))\n", "\n", "tail(cat_housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2adaaefe23aaaf2968fdc1699ef53565ce29adce", "_cell_guid": "ee8b8694-9497-4655-87a8-cb0eb1dae581"}, "source": ["## Scale the numerical variables\n", "\n", "Note here I scale every one of the numericals except for 'median_house_value' as this is what we will be working to predict. The x values are scaled so that coefficients in things like support vector machines are given equal weight, but the y value scale doen't affect the learning algorithms in the same way (and we would just need to re-scale the predictions at the end which is another hassle)."]}, {"cell_type": "code", "metadata": {"_uuid": "534bfb47d8990faae4937c6bb352715d65261245", "_cell_guid": "f92d34a8-3ff9-49cc-be16-3abdc95ce2f9"}, "execution_count": null, "outputs": [], "source": ["colnames(housing)"]}, {"cell_type": "code", "metadata": {"_uuid": "abb80b9d6473a0a5ba448904ee16a907aee49017", "_cell_guid": "d0b373ff-ecac-49ff-9801-6fc8ff3be22b"}, "execution_count": null, "outputs": [], "source": ["drops = c('ocean_proximity','median_house_value')\n", "housing_num =  housing[ , !(names(housing) %in% drops)]"]}, {"cell_type": "code", "metadata": {"_uuid": "bc428ca6d07505ac93e798cd7342617afe57dd41", "_cell_guid": "64143f99-a807-4b9c-b209-ac91afee7e13"}, "execution_count": null, "outputs": [], "source": ["head(housing_num)"]}, {"cell_type": "code", "metadata": {"_uuid": "40669cd2d21867b04c079320050a32faaa21744b", "_cell_guid": "fcd2379c-4fa7-492a-aa70-8590a4701d70"}, "execution_count": null, "outputs": [], "source": ["scaled_housing_num = scale(housing_num)"]}, {"cell_type": "code", "metadata": {"_uuid": "8cb5c87fe02e8851dc854028ca9dab65fb69e997", "_cell_guid": "ba880886-a666-43ef-b60a-2a25442062f3"}, "execution_count": null, "outputs": [], "source": ["head(scaled_housing_num)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f16a542eb841c22b45a987f773e2d69efba7330e", "_cell_guid": "b57366f6-ab73-487d-83e7-470969fc096e"}, "source": ["## Merge the altered numerical and categorical dataframes"]}, {"cell_type": "code", "metadata": {"_uuid": "4a3c7cf8fbd8b11f8c3e48d1da7bc7e67aae06b4", "_cell_guid": "fa53dae9-6094-417e-a6c6-101de3c98210"}, "execution_count": null, "outputs": [], "source": ["cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)"]}, {"cell_type": "code", "metadata": {"_uuid": "2a560c98f34f40a6082b1cf3c023b1ea4b681391", "_cell_guid": "404c2ed6-f634-4c7c-932a-1da644c795e5"}, "execution_count": null, "outputs": [], "source": ["head(cleaned_housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "c2e58952bd8add55a3a92ad3e18c8f6185d0ffe0", "_cell_guid": "15477c86-a6b1-4c54-9bb3-3cc6e0bf6dfc"}, "source": ["## Step 3. Create a test set of data\n", "We pull this subsection from the main dataframe and put it to the side to not be looked at prior to testing our models. Don't look at it, as snooping the test data introduces a bias to your work!\n", "\n", "This is the data we use to validate our model, when we train a machine learning algorithm the goal is usually to make an algorithm that predicts well on data it hasn't seen before. To assess this feature, we pull a set of data to validate the models as accurate/inaccurate once we have completed the training process."]}, {"cell_type": "code", "metadata": {"_uuid": "af5a54085e4356b5dee17676d2b205a4fd6ff751", "_cell_guid": "ac83be0b-8669-4843-a55f-aa4cf4c74360"}, "execution_count": null, "outputs": [], "source": ["set.seed(1738) # Set a random seed so that same sample can be reproduced in future runs\n", "\n", "sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace = F)\n", "train = cleaned_housing[sample, ] #just the samples\n", "test  = cleaned_housing[-sample, ] #everything but the samples"]}, {"cell_type": "markdown", "metadata": {"_uuid": "acf17d5f02783de97797c5b4ea9725aa82a4ea7d", "_cell_guid": "66d0bbb9-3929-4632-a840-fa8ae7fb3930"}, "source": ["I like to use little sanity checks like the ones below to make sure the manipulations have done what I want.\n", "With big dataframes you need find ways to be sure that don't involve looking at the whole thing every step!\n", "\n", "Note that the train data below has all the columns we want, and also that the index is jumbled (so we did take a random sample). The second check makes sure that the length of the train and test dataframes equals the length of the dataframe they were split from, which shows we didn't lose data or make any up by accident!"]}, {"cell_type": "code", "metadata": {"_uuid": "e0708cc9b9e55eb43a2c49db911cc9441601663b", "_cell_guid": "6b604970-a9ca-4f04-891d-c220c7c8b0d7"}, "execution_count": null, "outputs": [], "source": ["head(train)"]}, {"cell_type": "code", "metadata": {"_uuid": "498dbe0fbfbbfb70cb033612bfb3432cf5b470db", "_cell_guid": "07f35692-a6e1-4697-93c7-6fdbe137b934"}, "execution_count": null, "outputs": [], "source": ["nrow(train) + nrow(test) == nrow(cleaned_housing)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d1a4d432ec2272ad995031b92d6ff30d53a59d81", "_cell_guid": "7a9aa62a-8e88-457d-838a-16fb779d3eea"}, "source": ["## Step 4. Test some predictive models.\n", "\n", "We start here with just a simple linear model using 3 of the avaliable predictors. Median income, total rooms and population. This serves as an entry point to introduce the topic of cross validation and a basic model. We want a model that makes good predictions on data that it has not seen before. A model that explains the variation in the data it was trained on well, but does not generalize to external data is referred to as being overfit. You may thin \"that's why we split off some test data!\" but we don't want to repeatedly assess against our test set, as then the model can just become overfit to that set of data thus moving and not solving the problem. \n", "\n", "So here we do cross validation to test the model using the training data itself. Our K is 5, what this means is that the training data is split into 5 equal portions. One of the 5 folds is put to the side (as a mini test data set) and then the model is trained using the other 4 portions. After that the predictions are made on the folds that was withheld, and the process is repeated for each of the 5 folds and the average predictions produced from the iterations of the model is taken. This gives us a rough understanding of how well the model predicts on external data!"]}, {"cell_type": "code", "metadata": {"_uuid": "28e5e995fce922bd52029d1dce142964c2947107", "_cell_guid": "0082e67e-b2cf-4391-a01d-6b83f626e999"}, "execution_count": null, "outputs": [], "source": ["library('boot')"]}, {"cell_type": "code", "metadata": {"_uuid": "51ffd8e0a134b9d1ee6ddb02b4cb8c1225ba7330", "_cell_guid": "88140f01-d79b-4398-aa46-f033f6102cc1"}, "execution_count": null, "outputs": [], "source": ["?cv.glm # note the K option for K fold cross validation"]}, {"cell_type": "code", "metadata": {"_uuid": "134f78994ae3da7c3e97f09485b3220257c01bbc", "_cell_guid": "bc9de06a-3b30-49e7-a277-21125cf6f1bc"}, "execution_count": null, "outputs": [], "source": ["glm_house = glm(median_house_value~median_income+mean_rooms+population, data=cleaned_housing)\n", "k_fold_cv_error = cv.glm(cleaned_housing , glm_house, K=5)"]}, {"cell_type": "code", "metadata": {"_uuid": "5a601af77e3c0da0b6cf85c0bfcadae48579be11", "_cell_guid": "2aacc1ac-bcba-4023-8d46-04c8ce0561ba"}, "execution_count": null, "outputs": [], "source": ["k_fold_cv_error$delta"]}, {"cell_type": "markdown", "metadata": {"_uuid": "c1b27525834a0940063ca13255facfac326cbaca", "_cell_guid": "c7298214-e480-4eec-b95c-fc049c469350"}, "source": ["The first component is the raw cross-validation estimate of prediction error. \n", "The second component is the adjusted cross-validation estimate."]}, {"cell_type": "code", "metadata": {"_uuid": "3927469dec3f20d1a138043e52f8b7af0d729075", "_cell_guid": "18160f7c-7a70-45e4-83ef-8b69c7837823"}, "execution_count": null, "outputs": [], "source": ["glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]\n", "glm_cv_rmse #off by about $83,000... it is a start"]}, {"cell_type": "code", "metadata": {"_uuid": "559b4e01204bf1c290609bb74f76e41310fea829", "_cell_guid": "d5a24f1d-f180-4306-99e3-0a9307502ac7"}, "execution_count": null, "outputs": [], "source": ["names(glm_house) #what parts of the model are callable?"]}, {"cell_type": "code", "metadata": {"_uuid": "317400afdc079279ae8a9347d3ae5e5f5af02664", "_cell_guid": "a0cae04c-6ee8-4e0d-a54b-c43164ee2bb2"}, "execution_count": null, "outputs": [], "source": ["glm_house$coefficients "]}, {"cell_type": "markdown", "metadata": {"_uuid": "27ec8c77c459d29f6365a195757bd62e786dfe24", "_cell_guid": "c51988b2-41e5-459f-a108-256d5f1c684c"}, "source": ["Since we scaled the imputs we can say that of the three we looked at, median income had the biggest effect on housing price... but I'm always very careful and google lots before intrepreting coefficients!\n", "\n", "### Random forest model"]}, {"cell_type": "code", "metadata": {"_uuid": "e8512b4b831af044106b8cf91be398f3314dd665", "_cell_guid": "0ee5e07b-39e6-423a-899d-6329c6f23305"}, "execution_count": null, "outputs": [], "source": ["library('randomForest')"]}, {"cell_type": "code", "metadata": {"_uuid": "676df16515cdff93e199fcd869c25c9a7fd3d8b2", "_cell_guid": "da552181-6f0d-493d-80b7-89fc3c8db4d5"}, "execution_count": null, "outputs": [], "source": ["?randomForest"]}, {"cell_type": "code", "metadata": {"_uuid": "f83c81841ba15ca5fc510ae3ff6f4687fc09f2e8", "_cell_guid": "bb0c4c90-1ff4-4d30-95ff-effd33311df8"}, "execution_count": null, "outputs": [], "source": ["names(train)"]}, {"cell_type": "code", "metadata": {"_uuid": "b8fe3a4c01afb3d41af2b48b9ddd58518908c2ad", "_cell_guid": "24271b8d-aec8-4150-981c-f0a9676b21d2"}, "execution_count": null, "outputs": [], "source": ["set.seed(1738)\n", "\n", "train_y = train[,'median_house_value']\n", "train_x = train[, names(train) !='median_house_value']\n", "\n", "head(train_y)\n", "head(train_x)"]}, {"cell_type": "code", "metadata": {"_uuid": "60cbf36f6148c5d7bb78efccefc4566040211ccf", "_cell_guid": "ae92e669-eaf8-4ba8-93d0-2d253bf26258"}, "execution_count": null, "outputs": [], "source": ["#some people like weird r format like this... I find it causes headaches\n", "#rf_model = randomForest(median_house_value~. , data = train, ntree =500, importance = TRUE)\n", "rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)"]}, {"cell_type": "code", "metadata": {"_uuid": "76b1316eeea21676ee2f981630c4a4b45380dcbf", "_cell_guid": "48811683-d21f-4325-9f2a-a7a5764a3ed1"}, "execution_count": null, "outputs": [], "source": ["names(rf_model) #these are all the different things you can call from the model."]}, {"cell_type": "code", "metadata": {"_uuid": "9c3b370593a44dbe665ea6ad1759ebfc32ab7ba5", "_cell_guid": "e9d70225-ce3e-49de-965d-167dd91d265c"}, "execution_count": null, "outputs": [], "source": ["rf_model$importance"]}, {"cell_type": "markdown", "metadata": {"_uuid": "6e53433f31d318fb04cc46159352f2e1d2efb3fe", "_cell_guid": "730c84a2-ce8a-4804-aedf-7d52ba6897b4"}, "source": ["Percentage included mean squared error is a measure of feature importance. It is defined as the measure of the increase in mean squared error of predictions when the given variable is shuffled, thereby acting as a metric of that given variable\u2019s importance in the performance of the model. So higher number  ==  more important predictor.\n", "\n", "### The out-of-bag (oob) error estimate\n", "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n", "\n", "Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree."]}, {"cell_type": "code", "metadata": {"_uuid": "b8eb2192b53840218588a8fca69cd1b21fddf561", "_cell_guid": "152fdfe5-2165-4f5e-8346-4a34a9f61230"}, "execution_count": null, "outputs": [], "source": ["oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions"]}, {"cell_type": "code", "metadata": {"_uuid": "2f2affca903d4ed2e54573a9db73204f0a8406b8", "_cell_guid": "9427cf1b-af2b-4896-9c63-54d8491b2744"}, "execution_count": null, "outputs": [], "source": ["#you may have noticed that this is avaliable using the $mse in the model options.\n", "#but this way we learn stuff!\n", "train_mse = mean(as.numeric((oob_prediction - train_y)^2))\n", "oob_rmse = sqrt(train_mse)\n", "oob_rmse"]}, {"cell_type": "markdown", "metadata": {"_uuid": "8be82bc3f625dbb1f1fead60d3afe08ac60bab87", "_cell_guid": "0cbc5cca-e263-486c-8c1e-1cb9a8239328"}, "source": ["So even using a random forest of only 1000 decision trees we are able to predict the median price of a house in a given district to within $49,000 of the actual median house price. This can serve as our bechmark moving forward and trying other models.\n", "\n", "How well does the model predict on the test data?"]}, {"cell_type": "code", "metadata": {"_uuid": "4445a330bec4babe9974a224dd053b8fec3a0179", "_cell_guid": "d1a6fde4-6e45-4770-bd55-513d63f5ea5d"}, "execution_count": null, "outputs": [], "source": ["test_y = test[,'median_house_value']\n", "test_x = test[, names(test) !='median_house_value']\n", "\n", "\n", "y_pred = predict(rf_model , test_x)\n", "test_mse = mean(((y_pred - test_y)^2))\n", "test_rmse = sqrt(test_mse)\n", "test_rmse"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d962c0d8e0810163ebf34bb8d05f9d40149bc21a", "_cell_guid": "9ddf5d3c-7e1f-4e26-9d8f-4cce569de1bd"}, "source": ["Well that looks great! Our model scored roughly the same on the training and testing data, suggesting that it is not overfit and that it makes good predictions."]}, {"cell_type": "markdown", "metadata": {"_uuid": "1578c68fbd4a8775ec2cc2fa49e833d3c56a2a8b", "_cell_guid": "69af621e-2798-42e2-b59c-f885053c90ed"}, "source": ["## Step 5. Next Steps\n", "\n", "So above we have covered the basics of cleaning data and getting a machine learning algorithm up and running in R. But I've on purpose left some room for improvement.\n", "\n", "The obvious way to improve the model is to provide it with better data. Recall our columns:\n", "\n", "longitude\n", "latitude\n", "housing_median_age\n", "total_rooms\n", "total_bedrooms\n", "population\n", "households\n", "median_income\n", "median_house_value\n", "ocean_proximity\n", "\n", "### Suggestions on ways to improve the results\n", "Why not use your R skills to build new data! One suggestion would be to take the longitude and latitude and work with these data. You could try to find things like 'distance to closest city with 1 million people' or other location based stats. This is called feature engineering and data scientists get paid big bucks to do it effectively!\n", "\n", "You may also wish to branch out and try some other models to see if they improve over the random forest benchmark we have set. Note this is not an exhaustive list but a starting point\n", "\n", "Tree based methods:\n", "\n", "gradient boosting - library(gbm)\n", "extreme gradient boosting - library(xgb)\n", "\n", "Other fun methods:\n", "support vevtor machines - library(e1071)\n", "neural networks - library(neuralnet)\n", "\n", "### Hyperparameters and Grid search\n", "\n", "When tuning models the next thing to worry about is the hyperparameters. All this means is the different options you pass into a model when you initialze it. i.e. the hyperparameter in out random forest model was n_tree = x, we chose x = 500, but we could have tried x = 2500, x = 1500, x = 100000 etc. \n", "\n", "Grid search is a common method to find the best combination of hyperparameters (as there are often more than the 1 we see in the random forest example!). Essentially this is where you make every combination of a set of paramaters and run a cross validation on each set, seeing which set gives the best predictions. An alternative is random search. When the number of hyperparameters is high then the computational load of a full grid search may be too much, so a random search takes a subset of the combinations and finds the best one in the random sample (sounds like a crapshoot but it actually works well!). These methods can be implemented easily using a for loop or two... there are also packages avaliable to help with these tasks.\n", "\n", "Here we exit the scope of what I can cover in a short tutorial, look at the r package 'caret' it has great functions for streamling things like grid searches for the best parameters. http://caret.r-forge.r-project.org/\n", "\n", "\n", "## Have you made a sweet model that predicts well or taught you something?\n", "If so, you can submit the script to kaggle here: \n", "\n", "You can post a script or your own kernel (or fork this document and make a better version) up for the world to enjoy! I promise to upvote you if you do."]}, {"cell_type": "markdown", "metadata": {"_uuid": "f30dee583d730293a9962cfc71380e9dd67a3285", "_cell_guid": "a4411c1b-cd75-4aff-9143-e0b626c35559"}, "source": ["### Making your own models? Go forth with the train and test dataframes in hand to make your machine learn something!\n", "I have also followed up on this kernel with a few sequels. Take a look at some of the other kerels produced using this dataset! [Here I expand the model through the use of gradient boosting algorithms (also in r)](https://www.kaggle.com/camnugent/gradient-boosting-and-parameter-tuning-in-r) and [here I engineer some new features and increase the prediction accuracy even more (I did this one in python).](https://www.kaggle.com/camnugent/geospatial-feature-engineering-and-visualization)\n", "\n", "Also the code from this notebook has been refactored and made more R-like [here!](https://www.kaggle.com/karlcottenie/introduction-to-machine-learning-in-r-tutorial)"]}]}
